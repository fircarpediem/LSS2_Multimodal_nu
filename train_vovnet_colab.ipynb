{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33ad7fb3",
   "metadata": {},
   "source": [
    "# VoVNet + LSS v2 + Transformer Training on Google Colab\n",
    "\n",
    "**Multi-modal BEV Perception for Autonomous Driving**\n",
    "\n",
    "This notebook trains VoVNetV2-39 + LSS v2 + Lightweight Transformer on nu-A2D dataset.\n",
    "\n",
    "**Expected Performance:**\n",
    "- BEV mIoU: 52-54% (baseline: 47%)\n",
    "- Action F1: 75-78% (baseline: 72%)\n",
    "- Description F1: 71-74% (baseline: 68%)\n",
    "\n",
    "**Hardware Requirements:**\n",
    "- GPU: T4 (15GB) or better\n",
    "- RAM: 25GB+\n",
    "- Disk: 50GB+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b307ea5",
   "metadata": {},
   "source": [
    "## 1. Setup Conda Environment with Konda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cbb255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install konda for conda environment management in Colab\n",
    "!pip install -q konda\n",
    "import konda\n",
    "konda.install()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1518db8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify conda installation\n",
    "!conda --version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef8443",
   "metadata": {},
   "source": [
    "## 2. Clone Repository and Setup Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c4201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (if using Drive for data storage)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfa25d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your repository (replace with your repo URL)\n",
    "!git clone https://github.com/YOUR_USERNAME/Multimodal-XAD.git\n",
    "%cd Multimodal-XAD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f8d1ef",
   "metadata": {},
   "source": [
    "## 3. Create Conda Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9787dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conda environment from environment.yaml\n",
    "!conda env create -f environment.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99931ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate environment (konda method)\n",
    "import konda\n",
    "konda.activate('Multimodal_XAD')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9e8a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "!python -c \"import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}')\"\n",
    "!python -c \"import timm; print(f'timm: {timm.__version__}')\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a989a633",
   "metadata": {},
   "source": [
    "## 4. Prepare Dataset\n",
    "\n",
    "**Dataset Structure Required:**\n",
    "```\n",
    "data/trainval/nu-A2D-*/nu-A2D/trainval/\n",
    "├── v1.0-trainval/       # Metadata\n",
    "├── samples/             # Camera & LIDAR\n",
    "│   ├── CAM_FRONT/\n",
    "│   ├── CAM_BACK/\n",
    "│   ├── LIDAR_TOP/\n",
    "│   └── ...\n",
    "├── maps/                # Map masks\n",
    "├── action_all/\n",
    "├── desc_all/\n",
    "└── local_binmap/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b479e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Download from Google Drive\n",
    "# Replace with your actual data path\n",
    "!cp -r /content/drive/MyDrive/nu-A2D-dataset ./data/trainval/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb1eb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Upload dataset as ZIP and extract\n",
    "# Uncomment if uploading manually\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # Upload your nu-A2D-dataset.zip\n",
    "# !unzip -q nu-A2D-dataset.zip -d ./data/trainval/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d98da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data structure\n",
    "import os\n",
    "\n",
    "data_path = './data/trainval/nu-A2D-20260129T100537Z-3-001/nu-A2D/trainval'\n",
    "required_folders = ['v1.0-trainval', 'samples', 'maps', 'action_all', 'desc_all']\n",
    "\n",
    "print(\"Checking dataset structure...\")\n",
    "for folder in required_folders:\n",
    "    path = os.path.join(data_path, folder)\n",
    "    exists = os.path.exists(path)\n",
    "    print(f\"{'✓' if exists else '✗'} {folder}: {exists}\")\n",
    "\n",
    "# Count samples\n",
    "cam_path = os.path.join(data_path, 'samples/CAM_FRONT')\n",
    "if os.path.exists(cam_path):\n",
    "    num_samples = len([f for f in os.listdir(cam_path) if f.endswith('.jpg')])\n",
    "    print(f\"\\nTotal CAM_FRONT images: {num_samples}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ddfac6",
   "metadata": {},
   "source": [
    "## 5. Test Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22c705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test to verify model can be created\n",
    "!python src/model_vovnet_transformer.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55501ab3",
   "metadata": {},
   "source": [
    "## 6. Configure Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2284e323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "config = {\n",
    "    'vovnet_type': 'vovnet39',  # or 'vovnet57' for larger model\n",
    "    'pretrained': True,\n",
    "    'batch_size': 4,  # Reduce to 3 if OOM on T4\n",
    "    'epochs': 80,\n",
    "    'lr': 2e-4,\n",
    "    'num_workers': 2,  # Reduce for Colab\n",
    "    'save_dir': './checkpoints_vovnet_colab'\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb07a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory\n",
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c942b389",
   "metadata": {},
   "source": [
    "## 7. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4b32b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training script\n",
    "# This will train for 80 epochs with automatic checkpointing\n",
    "!python train_vovnet_transformer.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32155c87",
   "metadata": {},
   "source": [
    "## 8. Monitor Training (Run in Separate Cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e330dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor GPU usage during training\n",
    "!watch -n 1 nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3509bb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View training logs (run after training starts)\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "def parse_log(log_file='training.log'):\n",
    "    \"\"\"Parse training log and plot metrics\"\"\"\n",
    "    if not os.path.exists(log_file):\n",
    "        print(\"Log file not found. Training may not have started yet.\")\n",
    "        return\n",
    "    \n",
    "    epochs, train_loss, val_loss, bev_miou = [], [], [], []\n",
    "    \n",
    "    with open(log_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Epoch' in line and 'Train Loss' in line:\n",
    "                match = re.search(r'Epoch (\\d+).*Train Loss: ([\\d.]+).*Val Loss: ([\\d.]+).*BEV mIoU: ([\\d.]+)', line)\n",
    "                if match:\n",
    "                    epochs.append(int(match.group(1)))\n",
    "                    train_loss.append(float(match.group(2)))\n",
    "                    val_loss.append(float(match.group(3)))\n",
    "                    bev_miou.append(float(match.group(4)))\n",
    "    \n",
    "    if not epochs:\n",
    "        print(\"No training data found in log.\")\n",
    "        return\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "    \n",
    "    ax1.plot(epochs, train_loss, label='Train Loss', marker='o')\n",
    "    ax1.plot(epochs, val_loss, label='Val Loss', marker='s')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training & Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(epochs, bev_miou, label='BEV mIoU', marker='o', color='green')\n",
    "    ax2.axhline(y=47, color='r', linestyle='--', label='Baseline (47%)')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('mIoU (%)')\n",
    "    ax2.set_title('BEV Segmentation Performance')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Best BEV mIoU: {max(bev_miou):.2f}% at Epoch {epochs[bev_miou.index(max(bev_miou))]}\")\n",
    "\n",
    "# Call this periodically to monitor progress\n",
    "parse_log()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8cee9a",
   "metadata": {},
   "source": [
    "## 9. Resume Training (If Interrupted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c10c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find latest checkpoint\n",
    "import glob\n",
    "\n",
    "checkpoints = glob.glob('./checkpoints_vovnet_colab/checkpoint_epoch_*.pth')\n",
    "if checkpoints:\n",
    "    latest = max(checkpoints, key=os.path.getctime)\n",
    "    print(f\"Latest checkpoint: {latest}\")\n",
    "    print(f\"To resume, modify train_vovnet_transformer.py to load this checkpoint\")\n",
    "else:\n",
    "    print(\"No checkpoints found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c4f201",
   "metadata": {},
   "source": [
    "## 10. Save Results to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b6105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy checkpoints to Google Drive for persistence\n",
    "!mkdir -p /content/drive/MyDrive/Multimodal-XAD-Results\n",
    "!cp -r ./checkpoints_vovnet_colab /content/drive/MyDrive/Multimodal-XAD-Results/\n",
    "!cp training.log /content/drive/MyDrive/Multimodal-XAD-Results/ 2>/dev/null || echo \"No log file yet\"\n",
    "\n",
    "print(\"✓ Checkpoints saved to Google Drive!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1340e130",
   "metadata": {},
   "source": [
    "## 11. Evaluate Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b946a39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint and evaluate\n",
    "import torch\n",
    "from src.model_vovnet_transformer import compile_model_vovnet_transformer\n",
    "\n",
    "# Find best checkpoint (highest validation performance)\n",
    "best_ckpt = './checkpoints_vovnet_colab/best_model.pth'\n",
    "\n",
    "if os.path.exists(best_ckpt):\n",
    "    print(f\"Loading best checkpoint: {best_ckpt}\")\n",
    "    checkpoint = torch.load(best_ckpt)\n",
    "    \n",
    "    print(f\"\\nBest Model Performance:\")\n",
    "    print(f\"  Epoch: {checkpoint['epoch']}\")\n",
    "    print(f\"  BEV mIoU: {checkpoint['bev_iou']:.2f}%\")\n",
    "    print(f\"  Action F1: {checkpoint.get('action_f1', 0):.2f}%\")\n",
    "    print(f\"  Description F1: {checkpoint.get('desc_f1', 0):.2f}%\")\n",
    "else:\n",
    "    print(\"Best checkpoint not found. Training may still be in progress.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932c56cc",
   "metadata": {},
   "source": [
    "## 12. Quick Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf6486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference on a sample\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from src.model_vovnet_transformer import compile_model_vovnet_transformer\n",
    "\n",
    "# Load model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "grid_conf = {\n",
    "    'xbound': [-50.0, 50.0, 0.5],\n",
    "    'ybound': [-50.0, 50.0, 0.5],\n",
    "    'zbound': [-10.0, 10.0, 20.0],\n",
    "    'dbound': [4.0, 45.0, 1.0],\n",
    "}\n",
    "\n",
    "data_aug_conf = {\n",
    "    'resize_lim': (0.193, 0.225),\n",
    "    'final_dim': (128, 352),\n",
    "    'rot_lim': (-5.4, 5.4),\n",
    "    'H': 900, 'W': 1600,\n",
    "    'rand_flip': False,\n",
    "    'bot_pct_lim': (0.0, 0.22),\n",
    "    'cams': ['CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT',\n",
    "             'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT'],\n",
    "    'Ncams': 6,\n",
    "}\n",
    "\n",
    "model = compile_model_vovnet_transformer(\n",
    "    bsize=1, grid_conf=grid_conf, data_aug_conf=data_aug_conf,\n",
    "    outC=4, vovnet_type='vovnet39', pretrained=False\n",
    ").to(device)\n",
    "\n",
    "# Load weights\n",
    "if os.path.exists(best_ckpt):\n",
    "    model.load_state_dict(torch.load(best_ckpt)['model_state_dict'])\n",
    "    model.eval()\n",
    "    print(\"✓ Model loaded successfully!\")\n",
    "else:\n",
    "    print(\"⚠ No checkpoint found, using random weights\")\n",
    "\n",
    "# Dummy inference\n",
    "with torch.no_grad():\n",
    "    imgs = torch.randn(6, 3, 128, 352).to(device)\n",
    "    rots = torch.randn(1, 6, 3, 3).to(device)\n",
    "    trans = torch.randn(1, 6, 3).to(device)\n",
    "    intrins = torch.randn(1, 6, 3, 3).to(device)\n",
    "    post_rots = torch.randn(1, 6, 3, 3).to(device)\n",
    "    post_trans = torch.randn(1, 6, 3).to(device)\n",
    "    \n",
    "    bev_seg, action, desc = model(imgs, rots, trans, intrins, post_rots, post_trans)\n",
    "    \n",
    "    print(f\"\\nOutput shapes:\")\n",
    "    print(f\"  BEV Segmentation: {bev_seg.shape}\")\n",
    "    print(f\"  Action: {action.shape}\")\n",
    "    print(f\"  Description: {desc.shape}\")\n",
    "    \n",
    "    # Visualize BEV prediction\n",
    "    bev_pred = bev_seg.argmax(1).cpu().numpy()[0]\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(bev_pred, cmap='tab10')\n",
    "    plt.title('BEV Segmentation Prediction')\n",
    "    plt.colorbar(label='Class')\n",
    "    plt.xlabel('X (meters)')\n",
    "    plt.ylabel('Y (meters)')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aba3aa0",
   "metadata": {},
   "source": [
    "## 13. Clean Up (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354b5d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up disk space by removing intermediate checkpoints\n",
    "# (Keep only best and latest)\n",
    "import glob\n",
    "\n",
    "checkpoints = glob.glob('./checkpoints_vovnet_colab/checkpoint_epoch_*.pth')\n",
    "if len(checkpoints) > 5:\n",
    "    checkpoints.sort(key=os.path.getctime)\n",
    "    to_remove = checkpoints[:-5]  # Keep last 5\n",
    "    \n",
    "    for ckpt in to_remove:\n",
    "        os.remove(ckpt)\n",
    "        print(f\"Removed: {ckpt}\")\n",
    "    \n",
    "    print(f\"\\n✓ Cleaned up {len(to_remove)} old checkpoints\")\n",
    "else:\n",
    "    print(\"No cleanup needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5d43d5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tips for Google Colab:\n",
    "\n",
    "1. **Prevent Disconnection**: Keep browser tab active or use Colab Pro\n",
    "2. **Save Frequently**: Checkpoints auto-save every 5 epochs to Drive\n",
    "3. **Monitor GPU**: Use `!nvidia-smi` to check memory usage\n",
    "4. **Reduce Batch Size**: If OOM error, reduce from 4→3 or 3→2\n",
    "5. **Use T4 or Better**: Request GPU in Runtime → Change runtime type\n",
    "\n",
    "## Expected Timeline (T4 GPU):\n",
    "- ~15 min/epoch\n",
    "- 80 epochs = ~20 hours\n",
    "- Best results typically at epoch 60-70\n",
    "\n",
    "## Troubleshooting:\n",
    "- **OOM Error**: Reduce batch_size or use gradient checkpointing\n",
    "- **Slow Training**: Reduce num_workers to 0-2\n",
    "- **Data Not Found**: Check dataset structure in cell 4\n",
    "- **Import Errors**: Re-run conda environment setup"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
